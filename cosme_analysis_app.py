import pandas as pd
import requests
from bs4 import BeautifulSoup
import streamlit as st
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from textblob import TextBlob
import datetime
import time

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="ğŸ’„ @cosmeãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æã‚µã‚¤ãƒˆ", layout="wide", page_icon="ğŸ’–")

# ã‚«ã‚¹ã‚¿ãƒ CSSã§è£…é£¾
st.markdown("""
<style>
body {background-color: #FFF7F8;}
.stButton>button {background-color: #E91E63; color: #fff; border: none;}
.stSidebar .st-form {padding: 1em; background: #fff; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);}
h1, h2, h3 {color: #C2185B;}
.stDownloadButton>button {background-color: #C2185B; color: #fff;}
</style>
""", unsafe_allow_html=True)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
with st.sidebar:
    st.header("ğŸ” ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æ")
    st.write("@cosmeã®ãƒ¬ãƒ“ãƒ¥ãƒ¼URLã‚’å…¥åŠ›ã—ã€åˆ†æé–‹å§‹ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚")
    with st.form("sidebar_form"):
        url_input = st.text_input("ãƒ¬ãƒ“ãƒ¥ãƒ¼URL", placeholder="ä¾‹: https://www.cosme.net/products/10240630/review")
        submitted = st.form_submit_button("ğŸ” åˆ†æé–‹å§‹")
    st.markdown("---")

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
st.title("ğŸ’„ @cosmeãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æã‚µã‚¤ãƒˆ")
st.write("**å¹´ä»£ãƒ»è‚Œè³ªåˆ¥ã®è©•ä¾¡å‚¾å‘**ã€**æ„Ÿæƒ…åˆ†æ**ã€**ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°**ã€**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ**ã‚’ä¸€ç”»é¢ã§ç¢ºèªã§ãã¾ã™ã€‚")

# ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼†å…¨æ–‡å–å¾—å¯¾å¿œï¼‰
def get_reviews(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    reviews = []
    page = 1
    while True:
        # ãƒšãƒ¼ã‚¸ç•ªå·ä»˜ä¸
        paged_url = f"{url}?pageno={page}"
        resp = requests.get(paged_url, headers=headers)
        soup = BeautifulSoup(resp.text, "html.parser")
        items = soup.select(".c-section-review__item")
        if not items:
            break
        for item in items:
            # è©•ä¾¡
            star = item.select_one(".c-section-review__rating i")
            rating = None
            if star and star.has_attr('class'):
                for cls in star['class']:
                    if cls.startswith('star'):
                        try:
                            rating = int(re.sub(r'[^0-9]', '', cls))
                        except:
                            pass
            # å±æ€§
            profile = item.select_one(".c-section-review__profile")
            profile_txt = profile.get_text(strip=True) if profile else ""
            # ãƒ†ã‚­ã‚¹ãƒˆ
            excerpt = item.select_one(".c-section-review__text").get_text(strip=True) if item.select_one(".c-section-review__text") else ""
            more = item.select_one("a.c-section-review__readMore")
            full_txt = excerpt
            if more and more.has_attr('href'):
                # å…¨æ–‡ãƒšãƒ¼ã‚¸å–å¾—
                time.sleep(0.5)
                det = requests.get(more['href'], headers=headers)
                dsoup = BeautifulSoup(det.text, 'html.parser')
                detail = dsoup.select_one(".c-section-reviewDetail__text")
                if detail:
                    full_txt = detail.get_text(strip=True)
            # æ—¥ä»˜
            date_elem = item.select_one(".c-section-review__date")
            date_txt = date_elem.get_text(strip=True) if date_elem else ""
            reviews.append({"è©•ä¾¡": rating, "å±æ€§": profile_txt, "æœ¬æ–‡": full_txt, "æ—¥ä»˜": date_txt})
        page += 1
    return pd.DataFrame(reviews)

# åˆ†æå®Ÿè¡Œ
if submitted and url_input:
    with st.spinner("ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å–å¾—ä¸­..."):
        df = get_reviews(url_input)
    if df.empty:
        st.error("âš ï¸ ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚URLã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
    else:
        st.success(f"âœ… {len(df)} ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å–å¾—ã—ã¾ã—ãŸï¼")
        # å±æ€§åˆ†è§£
        df[["å¹´ä»£","è‚Œè³ª","æ€§åˆ¥"]] = df["å±æ€§"].str.extract(r"(\d+ä»£)?ãƒ»(.*?)ãƒ»(.*?)$")
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ“Š å¹´ä»£åˆ¥è©•ä¾¡å¹³å‡")
            fig1, ax1 = plt.subplots()
            df.groupby("å¹´ä»£")["è©•ä¾¡"].mean().plot(kind="bar", ax=ax1, edgecolor="black")
            ax1.set_ylabel("å¹³å‡è©•ä¾¡")
            st.pyplot(fig1)
        with col2:
            st.subheader("ğŸ“Š è‚Œè³ªåˆ¥è©•ä¾¡å¹³å‡")
            fig2, ax2 = plt.subplots()
            df.groupby("è‚Œè³ª")["è©•ä¾¡"].mean().plot(kind="bar", ax=ax2, edgecolor="black")
            ax2.set_ylabel("å¹³å‡è©•ä¾¡")
            st.pyplot(fig2)
        st.subheader("ğŸ˜Š æ„Ÿæƒ…åˆ†æï¼ˆãƒã‚¸/ãƒã‚¬åˆ†é¡ï¼‰")
        df["æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"] = df["æœ¬æ–‡"].apply(lambda x: TextBlob(x).sentiment.polarity)
        fig3, ax3 = plt.subplots()
        df["æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"].hist(bins=20, ax=ax3)
        ax3.set_xlabel("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢")
        ax3.set_ylabel("ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°")
        st.pyplot(fig3)
        st.subheader("ğŸ“ˆ è©•ä¾¡ã®æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰")
        df["ãƒ¬ãƒ“ãƒ¥ãƒ¼æ—¥"] = pd.to_datetime(df["æ—¥ä»˜"], errors='coerce')
        trend = df.dropna(subset=["ãƒ¬ãƒ“ãƒ¥ãƒ¼æ—¥"]).groupby("ãƒ¬ãƒ“ãƒ¥ãƒ¼æ—¥")["è©•ä¾¡"].mean()
        fig4, ax4 = plt.subplots()
        trend.plot(ax=ax4, marker='o')
        ax4.set_ylabel("å¹³å‡è©•ä¾¡")
        st.pyplot(fig4)
        st.subheader("ğŸ‘¥ å±æ€§åˆ¥ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°")
        tfidf = TfidfVectorizer(max_features=30, stop_words="japanese")
        X = tfidf.fit_transform(df["æœ¬æ–‡"])
        km = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X)
        df["ã‚¯ãƒ©ã‚¹ã‚¿"] = km.labels_
        st.dataframe(df[["å¹´ä»£","è‚Œè³ª","æ€§åˆ¥","è©•ä¾¡","ã‚¯ãƒ©ã‚¹ã‚¿"]])
        st.subheader("ğŸ” ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆå¹´ä»£Ã—ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰")
        seg = pd.crosstab(df["å¹´ä»£"], df["ã‚¯ãƒ©ã‚¹ã‚¿"])
        st.dataframe(seg)
        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        csv = df.to_csv(index=False).encode('utf-8-sig')
        st.download_button("CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="cosme_reviews.csv", mime="text/csv")
